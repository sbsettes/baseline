{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string"
      ],
      "outputs": [],
      "metadata": {
        "id": "dNfcnaSIE31y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_line(id,file_lines):\n",
        "    for line in file_lines:\n",
        "        if id in line:\n",
        "            return line"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def prepare_data(arguments,keypoints,labels):\n",
        "    if(arguments and keypoints and labels):\n",
        "        corpus = []\n",
        "        matches = []\n",
        "        for line in labels:\n",
        "            arg_id = line.split(\",\")[0]\n",
        "            keypoint_id = line.split(\",\")[1]\n",
        "            \n",
        "            match = int(line.split(\",\")[2])\n",
        "            argument = get_line(arg_id,arguments)\n",
        "            keypoint = get_line(keypoint_id,keypoints)\n",
        "            corpus.append(argument + \" \" + keypoint)\n",
        "            matches.append(match)\n",
        "    matches = array(matches)\n",
        "    return corpus,matches"
      ],
      "outputs": [],
      "metadata": {
        "id": "a7yhqqQ9E310",
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# tokenize and count words\n",
        "def tokenize_and_count(corpus):\n",
        "    if corpus:\n",
        "        all_words = []\n",
        "        for line in corpus:\n",
        "            table = str.maketrans(dict.fromkeys(string.punctuation))\n",
        "            line = line.translate(table)\n",
        "            line = line.lower()\n",
        "            try:\n",
        "                tokenize_word = word_tokenize(line)\n",
        "            except:\n",
        "                nltk.download('punkt')\n",
        "                tokenize_word = word_tokenize(line)\n",
        "            for word in tokenize_word:\n",
        "                all_words.append(word)\n",
        "        print (\"number of words in arguments+keypoints: \"+str(len(all_words)))\n",
        "        unique_words = list(dict.fromkeys(all_words))\n",
        "        vocab_length = len(unique_words) + 5\n",
        "        print (\"unique words: \" + str(vocab_length))\n",
        "        return vocab_length\n",
        "    else:\n",
        "        print(\"error while creating corpus\")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "6a0f29e0ba334a55bf38eafc2bb952c3",
            "ce2bc0eba0ef4a92bdf0397b6eb9824d",
            "566ceca92b45460b8971ec680df05333"
          ]
        },
        "id": "PTrgen0BE310",
        "outputId": "3fb80ebd-fbf3-49d1-8911-913f50088f9d",
        "scrolled": true,
        "tags": [
          "outputPrepend"
        ]
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_padded_sentences(corpus,vocab_length):\n",
        "    embedded_sentences = [one_hot(sent, vocab_length) for sent in corpus]\n",
        "    #print(embedded_sentences )\n",
        "\n",
        "    # we need the length of each embedded sentence to be the same\n",
        "    # so we calculate the length of the longest sentence embedding\n",
        "    word_count = lambda sentence: len(word_tokenize(sentence))\n",
        "    longest_sentence = max(corpus, key=word_count)\n",
        "    length_long_sentence = len(word_tokenize(longest_sentence))\n",
        "    print(\"length of the longest sentence: \" + str(length_long_sentence))\n",
        "\n",
        "    # now we add padding to the sentences that have a length smaller than length_long_sentence\n",
        "    padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
        "    print(padded_sentences)\n",
        "    return padded_sentences,length_long_sentence"
      ],
      "outputs": [],
      "metadata": {
        "id": "XbenxQ2OE311"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# simple one layer model to see if arguments and key points match or not\n",
        "def create_and_compile_model(vocab_length,length_long_sentence):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXEQq8jrE311",
        "outputId": "d1271fde-94e0-43ab-cb30-389e3885f22f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# train the model\n",
        "def train_model(model,padded_sentences,matches):\n",
        "    model.fit(padded_sentences, matches, epochs=100, verbose=1)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxldB26fE313",
        "outputId": "70d8fb1f-b6d3-4480-9a71-7421e18e00bb",
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# test the model\n",
        "def test_model(model,padded_sentences, matches):\n",
        "    loss, accuracy = model.evaluate(padded_sentences, matches, verbose=1)\n",
        "    print('Accuracy: %f' % (accuracy*100))\n",
        "    print('Loss: %f' % (loss*100))"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm_2WuAbE313",
        "outputId": "78b0963e-ef7c-4fa9-dde7-1eaa311848f3",
        "scrolled": false
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load training data\n",
        "with open(\"./data/train/arguments_train.csv\",\"r\",encoding=\"utf-8\") as f:\n",
        "    arguments_train = f.readlines()[1:]\n",
        "with open(\"./data/train/key_points_train.csv\",\"r\",encoding=\"utf-8\") as f:\n",
        "    keypoints_train = f.readlines()[1:]\n",
        "with open(\"./data/train/labels_train.csv\",\"r\",encoding=\"utf-8\") as f:\n",
        "    labels_train = f.readlines()[1:]\n",
        "\n",
        "print(\"keypoints training data size: \"+str(len(keypoints_train)))\n",
        "print(\"arguments training data size: \"+str(len(arguments_train)))\n",
        "print(\"labels training data size: \"+str(len(labels_train)))\n",
        "\n",
        "# train data embeddings\n",
        "corpus_train,matches_train = prepare_data(arguments_train,keypoints_train,labels_train)\n",
        "vocab_length_train = tokenize_and_count(corpus_train)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load test data\n",
        "with open(\"./data/test/arguments_dev.csv\",\"r\",encoding=\"utf-8\") as f:\n",
        "    arguments_dev = f.readlines()[1:]\n",
        "with open(\"./data/test/key_points_dev.csv\",\"r\",encoding=\"utf-8\") as f:\n",
        "    keypoints_dev = f.readlines()[1:]\n",
        "with open(\"./data/test/labels_dev.csv\",\"r\",encoding=\"utf-8\") as f:\n",
        "    labels_dev = f.readlines()[1:]\n",
        "\n",
        "print(\"keypoints test data size: \"+str(len(keypoints_dev)))\n",
        "print(\"arguments test data size: \"+str(len(arguments_dev)))\n",
        "print(\"labels test data size: \"+str(len(labels_dev)))\n",
        "\n",
        "# test data embeddings\n",
        "corpus_test,matches_test = prepare_data(arguments_dev,keypoints_dev,labels_dev)\n",
        "vocab_length_test = tokenize_and_count(corpus_test)\n",
        "\n",
        "\n",
        "if(vocab_length_test > vocab_length_train):\n",
        "    vocab_length_train = vocab_length_test\n",
        "else:\n",
        "    vocab_length_test = vocab_length_train\n",
        "\n",
        "print(\"generating train data embeddings.... \")\n",
        "padded_sentences_train,length_long_sentence_train = get_padded_sentences(corpus_train,vocab_length_train)\n",
        "\n",
        "print(\"generating test data embeddings.... \")\n",
        "padded_sentences_test,length_long_sentence_test = get_padded_sentences(corpus_test,vocab_length_test)\n",
        "\n",
        "if(length_long_sentence_test > length_long_sentence_train):\n",
        "    length_long_sentence_train = length_long_sentence_test\n",
        "else:\n",
        "    length_long_sentence_test = length_long_sentence_train"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# training phase\n",
        "\n",
        "model_train = create_and_compile_model(vocab_length_train,length_long_sentence_train)\n",
        "model_test = create_and_compile_model(vocab_length_test,length_long_sentence_test)\n",
        "\n",
        "train_model(model_train,padded_sentences_train,matches_train)"
      ],
      "outputs": [],
      "metadata": {
        "id": "gc-fa-M8E314"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"going to test model...\")\n",
        "test_model(model_test,padded_sentences_test,matches_test)"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmD2iTNKE314",
        "outputId": "ef592cde-dfb7-4c40-c8c2-1c0c9fbc7d57"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "baseline.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "metadata": {
      "interpreter": {
        "hash": "945ad770039b8ae403ce6948ef34191f16c77d73da0b6ce770f3b4c54fd3a82e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "566ceca92b45460b8971ec680df05333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a0f29e0ba334a55bf38eafc2bb952c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "Progress:",
            "description_tooltip": null,
            "layout": "IPY_MODEL_566ceca92b45460b8971ec680df05333",
            "max": 6758,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce2bc0eba0ef4a92bdf0397b6eb9824d",
            "value": 6758
          }
        },
        "ce2bc0eba0ef4a92bdf0397b6eb9824d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}